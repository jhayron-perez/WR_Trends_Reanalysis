{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8ffb02b-e77b-4eb6-8e69-f2b837ebbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "from numpy.polynomial import polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164e666c-23ce-485d-90dc-a328c3d4b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = '/glade/campaign/cgd/cesm/CESM2-LE/timeseries/atm/proc/tseries/day_1/Z500/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ec26c5-23df-4e92-8545-decf94c3e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = np.sort(glob.glob(f'{path_files}b.e21.BHISTsmbb*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994a2fc4-77ee-47fd-9a24-6616ac34ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_experiments_all = np.array(['.'.join(filenames[i].split('.')[4:-4]) for i in range(len(filenames))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c7afae-1a67-499a-92e1-81f85cb74ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names_experiments = np.unique(names_experiments_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "371779da-e821-4333-9ea1-d911c81cf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractz500_several_files(filestemp):\n",
    "    listxarrays = []\n",
    "    for file in filestemp:\n",
    "        dstemp = xr.open_dataset(file)\n",
    "        dstemp = dstemp.sel(time=slice('1940-01-01', '2014-12-31'))\n",
    "        dstemp = dstemp.Z500.where((dstemp.lat>=10),drop=True)\n",
    "        # Transpose the data to match the desired dimension order\n",
    "        dstemp_transposed = dstemp.transpose('time', 'lat', 'lon')\n",
    "        \n",
    "        # Recreate the DataArray with the coordinates in the desired order\n",
    "        dstemp_reordered = xr.DataArray(\n",
    "            dstemp_transposed.values,\n",
    "            dims=['time', 'lat', 'lon'],\n",
    "            coords={\n",
    "                'time': dstemp.coords['time'],\n",
    "                'lat': dstemp.coords['lat'],\n",
    "                'lon': dstemp.coords['lon']\n",
    "            },\n",
    "            attrs=dstemp.attrs,\n",
    "            name=dstemp.name\n",
    "        )\n",
    "        listxarrays.append(dstemp_reordered)\n",
    "    xarrayfull = xr.concat(listxarrays, dim='time')\n",
    "    cftime_index = xr.coding.cftimeindex.CFTimeIndex(xarrayfull['time'].values)\n",
    "    datetime_index = cftime_index.to_datetimeindex()\n",
    "    xarrayfull.coords['time'] = datetime_index\n",
    "    return xarrayfull.to_dataset()\n",
    "\n",
    "def fourierfilter(dataarray,cutoff_period=10):\n",
    "    # Compute the Fourier transform along the time axis\n",
    "    fft_data = np.fft.fft(dataarray, axis=0)\n",
    "    # Get the frequencies corresponding to the FFT components\n",
    "    freqs = np.fft.fftfreq(dataarray.shape[0], d=1)  # d=1 assumes daily data; adjust if different\n",
    "    \n",
    "    # Compute the corresponding periods (in days)\n",
    "    periods = np.abs(1 / freqs)\n",
    "    \n",
    "    # Define the cutoff period for high-pass filter (10 days)\n",
    "    cutoff_period = 10\n",
    "    \n",
    "    # Create a mask to filter out low-frequency components (longer than 10 days)\n",
    "    high_pass_mask = periods < cutoff_period\n",
    "    \n",
    "    # Apply the mask to the FFT data (set low-frequency components to zero)\n",
    "    fft_data_filtered = fft_data.copy()\n",
    "    fft_data_filtered[high_pass_mask, :, :] = 0\n",
    "    \n",
    "    # Perform the inverse FFT to get the filtered data back in the time domain\n",
    "    filtered_data = np.fft.ifft(fft_data_filtered, axis=0).real\n",
    "    \n",
    "    # Create a new xarray DataArray to store the filtered data\n",
    "    filtered_anomalies = xr.DataArray(\n",
    "        filtered_data,\n",
    "        dims=dataarray.dims,\n",
    "        coords=dataarray.coords,\n",
    "        attrs=dataarray.attrs\n",
    "    )\n",
    "    return filtered_anomalies\n",
    "\n",
    "def detrend_obs(data, train_data, npoly=3):\n",
    "    '''\n",
    "    detrend reanalysis using polynomial fit (for each doy) to the training mean\n",
    "    \n",
    "    data: [time, lat, lon] or [member, time]\n",
    "        reanalysis to detrend \n",
    "    \n",
    "    train_data: [time, lat, lon] or [time]\n",
    "        ensemble mean \n",
    "    \n",
    "    npoly: [int] \n",
    "        order of polynomial, default = 3rd order\n",
    "    '''\n",
    "    # stack lat and lon of ensemble mean data\n",
    "    if len(train_data.shape) == 3:\n",
    "        train_data = train_data.stack(z=('lat', 'lon'))\n",
    " \n",
    "    # stack lat and lon of member data & grab doy information\n",
    "    if len(data.shape) == 3:\n",
    "        data = data.stack(z=('lat', 'lon'))\n",
    "    temp = data['time.dayofyear']\n",
    "    \n",
    "    # grab every Xdoy from ensmean, fit npoly polynomial\n",
    "    # subtract polynomial from every Xdoy from members\n",
    "    detrend = []\n",
    "    for label,ens_group in train_data.groupby('time.dayofyear'):\n",
    "        Xgroup = data.where(temp == label, drop = True)\n",
    "        \n",
    "        curve = polynomial.polyfit(np.arange(0, ens_group.shape[0]), ens_group, npoly)\n",
    "        trend = polynomial.polyval(np.arange(0, ens_group.shape[0]), curve, tensor=True)\n",
    "        if len(train_data.shape) == 2: #combined lat and lon, so now 2\n",
    "            trend = np.swapaxes(trend,0,1) #only need to swap if theres a space dimension\n",
    "\n",
    "        diff = Xgroup - trend\n",
    "        detrend.append(diff)\n",
    "\n",
    "    detrend_xr = xr.concat(detrend,dim='time').unstack()\n",
    "    detrend_xr = detrend_xr.sortby('time')\n",
    "    \n",
    "    return detrend_xr\n",
    "\n",
    "def smooth_standard_deviation(std_doy, window=60):\n",
    "    # Extend the array by wrapping around for edge effects\n",
    "    extended_std_doy = xr.concat([std_doy[-window:], std_doy, std_doy[:window]], dim='dayofyear')\n",
    "    # Apply rolling mean and remove the extra days\n",
    "    smoothed_std = extended_std_doy.rolling(dayofyear=window, center=True).mean()\n",
    "    smoothed_std = smoothed_std[window:-window]\n",
    "    return smoothed_std\n",
    "\n",
    "def standardize_anomalies_with_smoothed_std(da):\n",
    "    # Compute day of year\n",
    "    doy = da['time'].dt.dayofyear\n",
    "    \n",
    "    # Group data by day of year and compute standard deviation\n",
    "    std_doy = da.groupby(doy).std('time')\n",
    "    \n",
    "    # Smooth the standard deviation using a 60-day rolling average\n",
    "    smoothed_std_doy = smooth_standard_deviation(std_doy, window=60)\n",
    "    # return smoothed_std_doy\n",
    "    # Standardize the anomalies by dividing by the smoothed standard deviation\n",
    "    standardized_da = da.groupby(doy) / smoothed_std_doy\n",
    "    \n",
    "    return standardized_da\n",
    "\n",
    "# # Example usage:\n",
    "# # standardized_anomalies = standardize_anomalies_with_smoothed_std(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4efc785e-a3d0-428e-acd5-8b678a3bb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/jhayron/tmp/ipykernel_4593/1649359784.py:25: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  datetime_index = cftime_index.to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "id_experiment = 0\n",
    "name_experiment = unique_names_experiments[id_experiment]\n",
    "where_files = np.where(names_experiments_all==name_experiment)[0]\n",
    "files_temp = filenames[where_files]\n",
    "dataset_temp = extractz500_several_files(files_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "728fb75b-f864-41dc-9158-53c7a4224516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anoms(dataset):\n",
    "    anoms = detrend_obs(dataset.Z500,dataset.Z500)\n",
    "    std_anoms = standardize_anomalies_with_smoothed_std(anoms)\n",
    "    filtered_anoms = fourierfilter(std_anoms)\n",
    "    filtered_anoms = filtered_anoms.to_dataset(name='Z_anoms')\n",
    "    filtered_anoms = filtered_anoms.drop_vars('dayofyear')\n",
    "    return filtered_anoms\n",
    "    # path_output_anoms = f'{path_origins}Z500Anoms_{name_reanalysis}_v2.nc'\n",
    "    # filtered_anoms.to_netcdf(path_output_anoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75b03-09a9-43ff-9cd2-f83277607169",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms_temp = compute_anoms(dataset_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c506b-6d8c-4e20-872f-75e5e21e6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_outputs_anoms = '/glade/derecho/scratch/jhayron/Data4WRsClimateChange/LENS_poly/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180b80d-c36c-4b0f-8b19-db70799173d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anoms_experiment_complete(id_experiment):\n",
    "    name_experiment = unique_names_experiments[id_experiment]\n",
    "    print(f'Started {name_experiment}')\n",
    "    where_files = np.where(names_experiments_all==name_experiment)[0]\n",
    "    files_temp = filenames[where_files]\n",
    "    dataset_temp = extractz500_several_files(files_temp)\n",
    "    \n",
    "    anoms_temp = compute_anoms(dataset_temp)\n",
    "    \n",
    "    filtered_anoms.to_netcdf(f'{path_outputs_anoms}anoms_{name_experiment}.nc')\n",
    "    print(f'Experiment {name_experiment} complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2814e1-7dc2-4f9d-9db8-400a14d8da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "num_ids = len(unique_names_experiments)\n",
    "num_processors = 8\n",
    "\n",
    "# Create a Pool of worker processes\n",
    "with Pool(processes=num_processors) as pool:\n",
    "    # Map the function to the range of IDs\n",
    "    pool.map(compute_anoms_experiment_complete, range(num_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
